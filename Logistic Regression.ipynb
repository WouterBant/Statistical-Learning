{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90886c4e",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109542f",
   "metadata": {},
   "source": [
    "#### When"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d9c9e",
   "metadata": {},
   "source": [
    "If the target variable is binary it is logical to model probabilities for the 2 events. \n",
    "\n",
    "When modelling probabilities it is not appropriate to use OLS, since this method does not take into account that probabilities need to be between 0 and 1. \n",
    "\n",
    "Often when you model probabilities it is better to use for example logistic regression or probit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ac4de",
   "metadata": {},
   "source": [
    "#### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e462081",
   "metadata": {},
   "source": [
    "$P[Y=1|x]=F(x^T\\beta)$, for probit $F(z)=\\Phi(z)$ and for logistic regression $F(z)=\\frac{e^z}{1+e^z}=\\frac{1}{e^{-z}+1}$, $z=h(x)=x^T\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2b10e",
   "metadata": {},
   "source": [
    "#### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2994a99",
   "metadata": {},
   "source": [
    "Logistic regression does not require distributional assumptions, it only requires that the log-odds function is linear in the features.\n",
    "\n",
    "Log-odds: $log(R)=log(\\frac{p_1}{p_0})=log(\\frac{\\frac{e^z}{1+e^z}}{\\frac{1}{1+e^z}})=z=x^T\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3cf1c",
   "metadata": {},
   "source": [
    "#### From Likelihood to Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27788a",
   "metadata": {},
   "source": [
    "From the model is easy to see that the likelihood function for logistic regression is given by: $L(\\beta)=\\prod_{i=1}^n(1-F(\\beta^Tx_i))^{(1-y_i)} \\text{ }F(\\beta^Tx_i)^{y_i}$.\n",
    "\n",
    "And from this it is easy to see that the log likelihood is given by: $l(\\beta)=\\sum_{i=1}^n(1-y_i)\\text{  }log((1-F(\\beta^Tx_i)))+ y_i \\text{ }log(F(\\beta^Tx_i))$.\n",
    "\n",
    "A logical approach is to maximize the (log) likelihood. One way to do this is by minimizing a cost function that is equal to -log likelihood:\n",
    "\n",
    "$J(\\beta)=-\\frac{1}{n}\\sum_{i=1}^n(1-y_i)\\text{  }log((1-F(\\beta^Tx_i)))+ y_i \\text{ }log(F(\\beta^Tx_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199baf5",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3aa101",
   "metadata": {},
   "source": [
    "This minimization of the loss function is here done by gradient descent.\n",
    "\n",
    "The update rule: $\\hat{\\beta_j}^{new} = \\hat{\\beta_j}^{old} - \\alpha \\frac{\\delta}{\\delta \\beta_j}J(\\beta)|_{\\beta = \\hat{\\beta}\\text{ }^{old}}=\\hat{\\beta_j}^{old} - \\alpha \\frac{1}{n}\\sum_{i=1}^n(\\frac{1}{e^{-x^T\\beta}+1}-y_{i,j})x_{i,j}|_{\\beta = \\hat{\\beta}\\text{ }^{old}}$, where $\\alpha$ is the learning rate.\n",
    "\n",
    "Which is the same as: $w_j = w_j - \\alpha \\cdot dw_j= w_j - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^n 2 x_{i,j} (\\hat{y}-y_i)$ and $b= b-\\alpha \\cdot db= b - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^n 2 (\\hat{y}-y_i)$, here b is sometimes called the bias or the estimated coefficient of the constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c78739",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd65f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d7502",
   "metadata": {},
   "source": [
    "#### Make Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efbde9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_test, y_pred):\n",
    "    acc = np.sum(y_test == y_pred)/len(y_test)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ceb6ef",
   "metadata": {},
   "source": [
    "#### Make Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052d2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr = 0.001, max_iters = 1000):\n",
    "        self.lr = lr\n",
    "        self.max_iters = max_iters\n",
    "        self.weights = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, p = X.shape\n",
    "        self.weights = np.zeros(p)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.max_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.b\n",
    "            y_pred = 1/(1 + np.exp(-linear_model))\n",
    "\n",
    "            dw = 1/n * np.dot(X.T, (y_pred - y))\n",
    "            db = 1/n * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.b\n",
    "        y_pred = 1/(1 + np.exp(-linear_model))\n",
    "        y_pred_cls = [1 if i >= 0.5 else 0 for i in y_pred]\n",
    "        return np.array(y_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6012a6",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e2b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "br = load_breast_cancer()\n",
    "X, y = br.data, br.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b782f2",
   "metadata": {},
   "source": [
    "#### Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888edb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.8596491228070176\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(lr=0.0001, max_iters=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"The accuracy is {Accuracy(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
