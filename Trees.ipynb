{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e041ac",
   "metadata": {},
   "source": [
    "# Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485169f",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100cd6f",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a49b0b",
   "metadata": {},
   "source": [
    "$f(x)=\\sum_{m=1}^M c_m \\mathbb{1}[x \\in R_m]$, for every observation that falls into region $R_k$ the prediction is $c_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c687b0",
   "metadata": {},
   "source": [
    "#### Prediction per region ($c_k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e41eb",
   "metadata": {},
   "source": [
    "One approach to get the values $c_m \\text{, } m \\in \\{1,...,M\\}$ is to minimize the mean squarred error function: \n",
    "\n",
    "$\\{\\hat{c}_m\\}= \\underset{c_m}{\\operatorname{argmin}}\\frac{1}{n} \\sum_{i=1}^n(y_i-\\sum_{m=1}^M c_m \\mathbb{1}[x \\in R_m])^2=\\sum_{m=1}^M(\\sum_{i:x_i\\in R_m}(y_i-c_m)^2) $, so for each m minmize $\\sum_{i:x_i\\in R_m}(y_i-c_m)^2$.\n",
    "\n",
    "The solution to this is well known, namely the mean: $\\hat{c}_m = \\frac{1}{|\\{i:x_i\\in R_m\\}|}\\sum_{i:x_i\\in R_m}y_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6a85a",
   "metadata": {},
   "source": [
    "#### Partitions ($R_k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d4a76",
   "metadata": {},
   "source": [
    "Note that you need to know the partitions $R_m \\text{, } m \\in \\{1,...,M\\}$ before you can detemine $c_m\\text{ for } m \\in \\{1,...M\\}$. One way to estimate these parameters is by using the CART algorithm. A stopping rule is important in this algorithm, otherwise the algorithm will isolate all observations and you overfit the data. Stopping early has as an advantage that the variance is low and as a disadvantage that the bias is high. The opposite is the case for stopping late."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4abaf",
   "metadata": {},
   "source": [
    "#### Aggregrate Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763b3ee",
   "metadata": {},
   "source": [
    "Trees can be unstable (a small change in the data can cause a large\n",
    "change in the final estimate). Unstable estimator suffers in large predictive variances, so a way to deal with this is by aggregating many decision trees to smooth the estimated regression function and reduce estimation variance. For regression this means that you average the target estimates over trees. \n",
    "\n",
    "One method that does this is Bagging (= Bootstrap aggregating). Resample $n^*$ data points randomly with replacement from the training data.\n",
    "\n",
    "A second method is Subagging (= Subsample aggregating). Resample $n^* < n$ times without replacement for each bootstrap dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a19dc",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7c414",
   "metadata": {},
   "source": [
    "The tree estimators are correlated since they all depend on the same sample. If the correlation is too high, law of large numbers is not working well and aggregation makes little improvements.\n",
    "\n",
    "To tackle this problem you can use Random Forest which takes only a subset of k features from the total set of p features  for each bootstrapped training set. Often $m \\approx \\sqrt{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fa6ca",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2862f",
   "metadata": {},
   "source": [
    "For classification 2 things are different:\n",
    "\n",
    "$\\bullet$ You maximize the posterior probability $P(y|x)\\text{, x }\\in R_m$ for each $m \\in \\{1,...,M\\}$ to determine $c_m \\in \\{1,...,M\\}$.\n",
    "\n",
    "$\\bullet$ You aggregrate over the posterior probabilities when you aggregrate trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea471aa",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba4351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695823e3",
   "metadata": {},
   "source": [
    "Make Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377f053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_test, y_pred):\n",
    "    acc = np.sum(y_test == y_pred)/len(y_test)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d174b",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec8d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1fbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbd68b",
   "metadata": {},
   "source": [
    "Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdf590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree_clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "Tree_clf = Tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bea9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4cd48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", Accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129636b5",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba6f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Without Random Forest: 0.9122807017543859\n",
      "Accuracy With Random Forest:    0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "br = load_breast_cancer()\n",
    "X, y = br.data, br.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "Tree_clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "Tree_clf = Tree_clf.fit(X_train, y_train)\n",
    "y_pred = Tree_clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy Without Random Forest:\", Accuracy(y_test, y_pred))\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy With Random Forest:   \", Accuracy(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
